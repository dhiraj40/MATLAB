 %%%%%%%%%%%%%%%%%%%%%  Layers specification %%%%%%%%%%%%%%%%%%
 
    %encoder
 	net.layers{end+1}.properties = struct('type','input', 'sizeFm' ,[28 28],'numFm',1 );         %inputLayer
	net.layers{end+1}.properties = struct('type','conv','numFm',12  ,'kernel',5,'pad',2,'Activation',@Relu,'dActivation', @dRelu);
    net.layers{end+1}.properties = struct('type','batchNorm','alpha',2^-6);
 	net.layers{end+1}.properties = struct('type','fc','numFm',10);
	
	%decoder
    net.layers{end+1}.properties = struct('type','fc','numFm',7*7,'Activation',@Relu,'dActivation', @dRelu);
    net.layers{end+1}.properties = struct('type','reshape', 'sizeFm' ,[7 7],'numFm',1);
	net.layers{end+1}.properties = struct('type','conv','numFm',12  ,'kernel',3,'pad',2,'Activation',@Relu,'dActivation', @dRelu);
    net.layers{end+1}.properties = struct('type','batchNorm','alpha',2^-6);
    net.layers{end+1}.properties = struct('type','fc','numFm',28*28);
    net.layers{end+1}.properties = struct('type','reshape', 'sizeFm' ,[28 28],'numFm',1);
    net.layers{end+1}.properties = struct('type','output','lossFunc',@MSE,'costFunc',@MSE_Cost);     %output layer - regression

 %%%%%%%%%%%%%%%%%%%%%  Hyper params - training %%%%%%%%%%%%%%%%%%
 
 	net.hyperParam.trainLoopCount = 1024;		%on how many images to train before evaluating the network
 	net.hyperParam.testImageNum   = 2048;   	% after each loop, on how many images to evaluate network performance
 	net.hyperParam.ni_initial     = 0.005;		% ni to start training process
 	net.hyperParam.ni_final       = 0.00005;	        % final ni to stop the training process

    net.runInfoParam.verifyBP     = 1; 
    net.hyperParam.batchNum       = 64;
    net.hyperParam.randomizeTrainingSamples = 0;
