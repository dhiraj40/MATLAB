 %%%%%%%%%%%%%%%%%%%%%  Layers specification %%%%%%%%%%%%%%%%%%
 
 	net.layers{end+1}.properties = struct('type','input', 'sizeFm' ,[28 28],'numFm',1 );         %inputLayer
	net.layers{end+1}.properties = struct('type','conv','numFm',12  ,'kernel',5,'pad',2);
 % 	net.layers{end+1}.properties = struct('type','batchNorm');
 	net.layers{end+1}.properties = struct('type','conv','numFm',24 ,'kernel',13);
  	net.layers{end+1}.properties = struct('type','batchNorm');
 	net.layers{end+1}.properties = struct('type','fc','numFm',128, 'dropOut' ,0.8);
 	net.layers{end+1}.properties = struct('type','fc','numFm',10);
  	net.layers{end+1}.properties = struct('type','softmax');
	net.layers{end+1}.properties = struct('type','output','lossFunc',@CrossEnt,'costFunc',@CrossEnt_Cost);     %output Layer - classification
 
 %%%%%%%%%%%%%%%%%%%%%  Hyper params - training %%%%%%%%%%%%%%%%%%
 
 	net.hyperParam.trainLoopCount = 2048;		  % on how many images to train before evaluating the network
 	net.hyperParam.testImageNum   = 1024;   	  % after each loop, on how many images to evaluate network performance
 	net.hyperParam.ni_initial     = 0.1;		  % ni (step size) to start training process
 	net.hyperParam.ni_final       = 0.0005;	      % final ni to stop the training process
 	net.hyperParam.noImprovementTh= 8;            % after how many iterations with no loss improvment to update ni

 	net.hyperParam.batchNum       = 16;
 	net.hyperParam.normalizeNetworkInput    = 1;  %will normalize every samples with mean 0, var=1 before passing to net
 	net.hyperParam.randomizeTrainingSamples = 0;  %shuffle train set

 	net.runInfoParam.verifyBP               = 1;  % gradient verification
