%%%%%%%%%%%%%%%%%%%%%  Layers specification %%%%%%%%%%%%%%%%%%

 	net.layers{end+1}.properties = struct('type','input', 'sizeFm' ,[28 28 28],'numFm',1 );         %inputLayer
	net.layers{end+1}.properties = struct('type','conv','numFm',7  , 'Activation',@Relu, 'dActivation',@dRelu,'kernel',5,'pad',2, 'stride', [2 2 4], 'pooling', [1 1 1]);
%	net.layers{end+1}.properties = struct('type','batchNorm', 'initGamma',1, 'initBeta',0, 'alpha' , 2^-5 );
	net.layers{end+1}.properties = struct('type','conv','numFm',17 , 'kernel',[5 5 3]  ,'pad',[1 1 0], 'pooling', [1 1 1]);
	net.layers{end+1}.properties = struct('type','batchNorm'); % using defaults
	net.layers{end+1}.properties = struct('type','fc','numFm',128, 'dropOut' ,0.8);
	net.layers{end+1}.properties = struct('type','fc','numFm',10);
	net.layers{end+1}.properties = struct('type','softmax');
 	net.layers{end+1}.properties = struct('type','output','lossFunc',@CrossEnt,'costFunc',@CrossEnt_Cost);     %output Layer - classification

%%%%%%%%%%%%%%%%%%%%%  Hyper params - training %%%%%%%%%%%%%%%%%%

	net.hyperParam.trainLoopCount = 500;		%on how many images to train before evaluating the network
	net.hyperParam.testImageNum   = 300;   	    % after each loop, on how many images to evaluate network performance
	net.hyperParam.ni_initial     = 0.1;		% ni to start training process
	net.hyperParam.ni_final       = 0.0001;	    % final ni to stop the training process
    net.runInfoParam.verifyBP     = 1;
	net.hyperParam.noImprovementTh= 8;          % after how many iterations with no loss improvment to update ni
	net.hyperParam.normalizeNetworkInput = 1;   % will normalize every samples with mean 0, var=1 before passing to net

    net.hyperParam.batchNum=10;
